# Docker Builder Service

Небольшое приложение на FastAPI для клонирования репозитория, сборки Docker-образа и опциональной отправки в реестр.

Цель этого репозитория — предоставить HTTP API для автоматической сборки образов. Этот README объясняет, как контейнеризовать и запускать сервис, а также какие нюансы учитывать при том, что само приложение вызывает Docker (git, docker, gcloud и т.п.).

Коротко о содержимом

- `app/` — исходники приложения (FastAPI)
- `tests/` — базовые тесты (используют dry_run)
- `requirements.txt` — зависимости Python

Можно ли запускать приложение в Docker?

Да. Приложение можно упаковать в Docker-образ и запускать в ко��тейнере. Однако приложение само запускает системные команды (`git`, `docker`, опционально `gcloud`), поэтому чтобы оно могло управлять сборкой/пушем образов, нужно предоставить доступ к Docker-демону (или установить и запустить Docker внутри контейнера — Docker-in-Docker).

Два рекомендуемых варианта:

1) Подключение к Docker-демону хоста (рекомендуемое для удобства):
   - Примонтировать Unix-сокет `/var/run/docker.sock` в контейнер: контейнер использует docker CLI (клиент) и посылает команды демону хоста.
   - Плюсы: просто, быстро, не требует запуска dind.
   - Минусы: контейнер получает полномочия демона Docker хоста (высокий уровень доступа).

2) Docker-in-Docker (dind) через docker-compose (sidecar):
   - Запустить официальный `docker:dind` сервис и соединить приложение с ним (или использовать его как отдельный ��емон).
   - Плюсы: изоляция от демона хоста.
   - Минусы: нужно запускать `privileged` сервис, добавляется сложность и потенциальная накладная на производительность.

Учтите также:
- `gcloud` (Google Cloud SDK) не установлен в образ по-умолчанию. Если вы хотите использовать `gcloud auth print-access-token` для логина в GCR/Artifact Registry, установите SDK в образ или пробросьте соответствующие учётные данные/токены через секреты/переменные окружения.
- Для доступа к приватным реестрам рекомендуется использовать безопасную передачу учётных данных (через переменные окружения, Docker secrets, GCP Secret Manager). Не храните пароли в репозитории.

Что я добавил

- `Dockerfile` — образ на базе `python:3.11-slim`, с установкой `git` и `docker` CLI и установкой Python-зависимостей из `requirements.txt`.
- `docker-compose.yml` — демонстрационный файл с двумя опциями: с примонтированным Docker-сокетом хоста и с сервисом dind (можно использовать по необходимости).
- `.dockerignore` — чтобы не копировать лишние файлы в образ.
- `README.md` — этот файл.

Как собрать и запустить (локально)

1) Собрать образ локально и запустить с примонтированным Docker-сок��том:

```bash
# собрать образ
docker build -t docker-builder-service .

# запустить контейнер (примеры)
# Вариант A: использовать Docker хоста (простой)
docker run --rm -p 8000:8000 -v /var/run/docker.sock:/var/run/docker.sock -v "$PWD":/app docker-builder-service

# Вариант B: использовать docker-compose (включён сервис dind)
docker-compose up --build
```

2) В контейнере сервис доступен на http://localhost:8000

API

- POST /api/build — создать задачу сборки. Формат payload совпадает с Pydantic-моделью `CreateBuildPayload` в `app/main.py`.
- GET /api/build/{job_id} — получить статус и логи
- GET /api/builds — список задач

Пример payload (JSON):

```json
{
  "repo_url": "https://github.com/example/repo.git",
  "branch": "main",
  "tag": "v1.0.0",
  "registry": "example.registry/repo/image",
  "dockerfile_path": "Dockerfile",
  "push": true,
  "dry_run": true
}
```

Запуск тестов (локально, в venv)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pytest -q
```

Безопасность и рекомендации

- Примонтированный `/var/run/docker.sock` даёт контейнеру высокий уровень контроля над Docker-демоном хоста — для production-окружений стоит рассмотреть изоляцию, RBAC, или отдельные билд-агенты.
- Для приватных реестров используйте менеджер секретов (напри��ер, GCP Secret Manager, HashiCorp Vault) или системны�� переменные окружения, а не храните пароли прямо в запросах.
- Рассмотрите запуск билд-работ в отдельном изолированном билдере (CI/CD), а не прямо в сервисе, если вам важна безопасность.

Дальше (опционально)

- Добавить health endpoint.
- Настроить логирование в файл/JSON и ротацию.
- Добавить инструкцию по деплою в Kubernetes (с описанием способа доступа к Docker или использованию Kaniko/BuildKit для сборок внутри кластера).

## Jobs, воркеры и очереди (observability)

Ниже — краткое и практичное руководство по тому, как в этом проекте устроены задачи (jobs), очереди и воркеры, где искать логи и как отлаживать процесс исполнения задач.

### Ключевые понятия
- Job (метаданные) — запись о задаче, которую мы показываем в UI: `id`, `state` (queued/running/done/error), `logs` и т.д. Это «источник правды» для мониторинга.
- Queued task (задача в очереди) — сериализованный объект задачи, который хранит очередь (в данном проекте — RQ в Redis). Пока задача в очереди, она ожидает воркера.
- Worker — процесс, который берёт задачу из очереди и выполняет её, обновляя job-метаданные.

### Где хранятся job-метаданные
- В проекте используется абстракция `JobStore` (`app/store.py`) с двумя реализациями:
  - `InMemoryJobStore` — хранит jobs в памяти Python-процесса (fallback, если Redis не настроен).
  - `RedisJobStore` — хранит jobs в Redis (используется при нал��чии `REDIS_URL` и библиотеки `redis.asyncio`).

Когда job создаётся, код вызывает `store.create_job(job_id, "queued", [])`. Логи и состояние обновляются через `store.append_log` и `store.set_state`.

### Структура данных в Redis (если включён)
- `jobs:ids` — множество всех id задач.
- `job:<JOB_ID>` — hash с полями (например `id`, `state`, `logs` — где `logs` обычно хранится как JSON-массив строк).
- `deploy:<JOB_ID>` — в случае deploy-операций проект дополнительно сохраняет служебную информацию (mappings, user и т.д.).
- Пользовательские данные: `user:{sub}:registries`, `user:{sub}:creds`, `user:{sub}:history`, `user:{sub}:service_mappings`.

### Очередь RQ и воркер
- Если в окружении доступны sync-Redis и пакет `rq`, приложение инициализирует `_queue = Queue(connection=redis_sync.from_url(REDIS_URL))`.
- При создании билда/деплоя код пытается положить задачу в RQ: `_queue.enqueue('app.worker.process_build', req.dict(), job_timeout=3600)`.
- `app/worker.py` содержит `process_build` — точку входа для RQ worker. Воркеры запускаются отдельно и выполняют задачу, обновляя `job:<id>` через `JobStore`.
- Если RQ или Redis недоступны, приложение использует fallback: `asyncio.create_task(_run_build(req))` (или `_run_deploy`) и запускает задачу в том же процессе (uvicorn). В этом случае обычно используется `InMemoryJobStore`.

### Жизненный цикл job (схематично)
1. Клиент делает POST `/api/build` или `/api/deploy-services`.
2. Сервер генерирует `job_id` и вызывает `store.create_job(job_id, "queued", [])`.
3. Сервер пытается enqueue в RQ. Если enqueue успешен — задача ждёт воркера. Если нет — запускается background task в том же процессе.
4. Когда воркер/фон task начинает выполнение — `store.set_state(job_id, "running")`.
5. Во время выполнения вывод (stdout) задачи записывается через `store.append_log(job_id, line)`; UI читает эти логи.
6. По завершении ставится `done` или `error`.

### Как смотреть логи и статус (полезные команды)
- Список job id (Redis):
```bash
redis-cli SMEMBERS jobs:ids
# или с URL
redis-cli -u "${REDIS_URL}" SMEMBERS jobs:ids
```

- Просмотр job-метаданных (Redis):
```bash
redis-cli HGETALL "job:<JOB_ID>"
# только логи
redis-cli HGET "job:<JOB_ID>" logs | jq
```

- Просмотр deploy-метаданных:
```bash
redis-cli HGETALL "deploy:<JOB_ID>"
```

- Если вы используете RQ, запустите воркер вручную (в окружении с REDIS_URL):
```bash
export REDIS_URL="redis://localhost:6379/0"
rq worker   # запускает воркер, логи видны в этом терминале
```
- Полезные инструменты для RQ: `rq info` и `rq-dashboard` (веб UI для очереди).

- Через API / UI:
  - Swagger: `http://localhost:8000/docs` — можно запросить `/api/builds` и `/api/build/{job_id}`.
  - Frontend: `http://localhost:3000/#/queue` — список задач и live-логи (UI polling реализован в `frontend/src/components/Queue.jsx`).

### Fallback: что если Redis/ RQ нет
- В этом режиме `InMemoryJobStore` хранит метаданные в памяти процесса. Задачи могут исполняться в том же процессе (uvicorn) как asyncio background tasks. Ограничения:
  - После перезапуска сервиса все in-memory job-логи и состояния теряются.
  - Нет распределённой очереди: одна нода выполняет всё.

Рекомендуется иметь локально Redis + отдельный RQ worker для корректного поведения и надёжного хранения логов.

### Отмена / прерывание задач
- В текущей реализации явного API для отмены нет. Подходы для добавления cancel:
  - Если задача выполняется как subprocess (например `ansible-playbook`), воркер может запоминать PID процесса и при запросе на отмену делать `proc.terminate()` / `kill`.
  - Если задача выполняется в asyncio task внутри процесса, можно хранить ссылку на `asyncio.Task` и вызывать `task.cancel()`.
  - Также можно хранить флаг отмены в Redis (например `cancel:<JOB_ID>`) — воркер проверяет флаг и завершает работу аккуратно.

Будьте осторожны: прерывание внешних инструментов (git/docker/ansible) может оставить промежуточные артефакты.

### Рекомендации и best practices
- Для разработки: поднять Redis локально и запускать `rq worker` в отдельном терминале — так логи и история будут сохраняться и доступны после рестартов сервиса.
- Для продакшена: используйте Redis с ретеншеном логов или внешний лог-стор (ELK, Cloud Logging) и мониторинг воркеров (supervisord/systemd + `rq-dashboard`).
- Для надёжности: задавайте timeouts и job-time-limits (RQ поддерживает `job_timeout`), реализуйте ретраи для временных ошибок.

### Быстрый чеклист для отладки
- Запустите Redis, если нет:
```bash
sudo apt install redis-server
sudo service redis-server start
```
- Запустите backend (uvicorn) и фронтенд (vite).
- Запустите RQ воркер:
```bash
export REDIS_URL="redis://localhost:6379/0"
rq worker
```
- Создайте job через UI или curl и наблюдайте в `#/queue` и через Redis-ключи.

---

Автор: добавлен автоматически по запросу.
